# -*- coding: utf-8 -*-
"""
Banking ML Project - Streamlit Demo Application (∆Øu ti√™n 1 + 2 + 3)

∆ØU TI√äN 1 (Stability):
- Check file t·ªìn t·∫°i + schema t·ªëi thi·ªÉu
- Fallback an to√†n n·∫øu account kh√¥ng c√≥ cluster/recommendation/anomaly
- Kh√¥ng crash do .iloc[0]
- T·ª± merge AccountID v√†o anomalies n·∫øu file outputs/anomalies.csv b·ªã thi·∫øu

∆ØU TI√äN 2 (UX):
- B·ªô l·ªçc: Date range, TransactionType, RiskLevel, Min/Max amount, Toggle anomalies/all
- Charts: Timeline amount + highlight anomalies, Distribution AnomalyScore (customer vs global), Customer vs Cluster bar
- Download button: anomalies/profile/recommendation
- Feedback loop: "Not Fraud" => outputs/feedback.csv

∆ØU TI√äN 3 (Model Layer & Explainability):
- Balance Trend (Linear Regression)
- Goal-based savings tab
- Deviation Explanation for anomalies
"""

from pathlib import Path
from datetime import datetime
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression


# Optional
try:
    import plotly.express as px
except Exception:
    px = None


# =========================
# Page config
# =========================
st.set_page_config(page_title="Ph√¢n t√≠ch ML Ng√¢n h√†ng", page_icon="üè¶", layout="wide")


# =========================
# Helpers
# =========================
def to_bool_series(s: pd.Series) -> pd.Series:
    """Robust convert IsAnomaly to bool, support True/False, 0/1, 'true'/'false'."""
    if s is None:
        return pd.Series([], dtype=bool)

    if pd.api.types.is_bool_dtype(s):
        return s.fillna(False)

    if pd.api.types.is_numeric_dtype(s):
        return s.fillna(0).astype(int).eq(1)

    txt = s.astype(str).str.strip().str.lower()
    return txt.isin(["true", "1", "yes", "y", "t"])


def safe_get_row(df: pd.DataFrame, key_col: str, key_val):
    """Return first matching row as Series, or None."""
    if df is None or df.empty or key_col not in df.columns:
        return None
    m = df[key_col] == key_val
    if not m.any():
        return None
    return df.loc[m].iloc[0]


def safe_datetime_minmax(series: pd.Series):
    s = pd.to_datetime(series, errors="coerce").dropna()
    if s.empty:
        return None, None
    return s.min(), s.max()


def risk_level_order(level: str) -> int:
    order = {"High": 0, "Medium": 1, "Low": 2}
    return order.get(str(level), 99)


def require_columns(df: pd.DataFrame, required: list, df_name: str) -> bool:
    if df is None:
        st.error(f"‚ùå `{df_name}` ƒëang None.")
        return False
    missing = [c for c in required if c not in df.columns]
    if missing:
        st.error(f"‚ùå `{df_name}` thi·∫øu c·ªôt b·∫Øt bu·ªôc: {missing}.")
        return False
    return True


def csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False).encode("utf-8-sig")


def append_feedback(base_path: Path, rows: list[dict]):
    """Append feedback rows to outputs/feedback.csv (create if not exists)."""
    out_path = base_path / "outputs" / "feedback.csv"
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fb_new = pd.DataFrame(rows)
    if out_path.exists():
        try:
            fb_old = pd.read_csv(out_path)
            fb = pd.concat([fb_old, fb_new], ignore_index=True)
        except Exception:
            fb = fb_new
    else:
        fb = fb_new
    fb.to_csv(out_path, index=False, encoding="utf-8-sig")


def _strip_cols(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    df.columns = df.columns.astype(str).str.strip()
    return df

# --- New helpers for Priority 3 ---

def compute_balance_trend(tx_df: pd.DataFrame, account_id: str):
    """
    Return dict: slope_per_day, slope_per_month, r2, last_balance, forecast_30d, forecast_90d, model_df
    """
    if tx_df is None or tx_df.empty:
        return None

    need_cols = {"AccountID", "TransactionDate", "AccountBalance"}
    if not need_cols.issubset(tx_df.columns):
        return None

    df = tx_df.copy()
    df["AccountID"] = df["AccountID"].astype(str).str.strip()
    df = df[df["AccountID"] == str(account_id)].copy()

    df["TransactionDate"] = pd.to_datetime(df["TransactionDate"], errors="coerce")
    df["AccountBalance"] = pd.to_numeric(df["AccountBalance"], errors="coerce")

    df = df.dropna(subset=["TransactionDate", "AccountBalance"])
    if df.empty or df["TransactionDate"].nunique() < 2:
        return None

    # Use last balance per day (reduce noise)
    df = df.sort_values("TransactionDate")
    daily = df.groupby(df["TransactionDate"].dt.date, as_index=False).tail(1).copy()
    daily["Date"] = pd.to_datetime(daily["TransactionDate"].dt.date)

    t0 = daily["Date"].min()
    daily["t"] = (daily["Date"] - t0).dt.days.astype(int)

    X = daily[["t"]].values
    y = daily["AccountBalance"].values

    model = LinearRegression()
    model.fit(X, y)
    r2 = float(model.score(X, y))

    slope_per_day = float(model.coef_[0])
    slope_per_month = slope_per_day * 30.0

    last_date = daily["Date"].max()
    last_balance = float(daily.loc[daily["Date"] == last_date, "AccountBalance"].iloc[0])

    def pred(days_ahead: int) -> float:
        t_pred = int((last_date - t0).days + days_ahead)
        return float(model.predict(np.array([[t_pred]]))[0])

    return {
        "slope_per_day": slope_per_day,
        "slope_per_month": slope_per_month,
        "r2": r2,
        "last_balance": last_balance,
        "last_date": last_date,
        "forecast_30d": pred(30),
        "forecast_90d": pred(90),
        "daily_series": daily[["Date", "AccountBalance"]].rename(columns={"AccountBalance": "Balance"}),
    }

def deviation_explanation(tx_row: pd.Series, customer_hist: pd.DataFrame, feature_cols=None, top_k=3):
    """
    Return list of explanations like:
    [{"feature":"LoginAttempts","value":3,"baseline":1.1,"z":2.4,"reason":"cao b·∫•t th∆∞·ªùng"}, ...]
    """
    if feature_cols is None:
        feature_cols = ["TransactionAmount", "TransactionDuration", "LoginAttempts", "AccountBalance"]
        if "TimeBetweenTransactions" in customer_hist.columns:
            feature_cols.append("TimeBetweenTransactions")

    hist = customer_hist.copy()
    out = []
    for f in feature_cols:
        if f not in hist.columns or f not in tx_row.index:
            continue
        h = pd.to_numeric(hist[f], errors="coerce").dropna()
        x = pd.to_numeric(pd.Series([tx_row.get(f)]), errors="coerce").iloc[0]
        if h.empty or pd.isna(x):
            continue

        mu = float(h.mean())
        sd = float(h.std()) if float(h.std()) > 1e-9 else None

        if sd is None:
            z = 0.0
        else:
            z = float((x - mu) / sd)

        direction = "cao" if x > mu else "th·∫•p"
        reason = f"{direction} b·∫•t th∆∞·ªùng so v·ªõi th√≥i quen (mean={mu:.2f}, std={sd if sd else 0:.2f})"
        out.append({"feature": f, "value": float(x), "baseline_mean": mu, "z": z, "reason": reason})

    # rank by |z|
    out = sorted(out, key=lambda d: abs(d["z"]), reverse=True)
    return out[:top_k]


# =========================
# Load data
# =========================
@st.cache_data(show_spinner=False)
def load_data():
    base_path = Path(__file__).parent.parent

    paths = {
        "clusters": base_path / "outputs" / "clusters.csv",
        "anomalies": base_path / "outputs" / "anomalies.csv",
        "recommendations": base_path / "outputs" / "recommendations.csv",
        "transactions": base_path / "bank_transactions_data_2.csv",
    }

    # 1) Check files exist
    missing_files = [k for k, p in paths.items() if not p.exists()]
    if missing_files:
        st.error(
            "‚ùå Thi·∫øu file d·ªØ li·ªáu: " + ", ".join(missing_files)
            + "\n\n‚û°Ô∏è H√£y ƒë·∫£m b·∫£o c√°c file t·ªìn t·∫°i ƒë√∫ng ƒë∆∞·ªùng d·∫´n trong project."
        )
        st.stop()

    # 2) Load
    clusters_df = pd.read_csv(paths["clusters"])
    anomalies_df = pd.read_csv(paths["anomalies"])
    recs_df = pd.read_csv(paths["recommendations"])
    tx_df = pd.read_csv(paths["transactions"])

    # 3) Strip column names (avoid hidden spaces/BOM)
    for df in (clusters_df, anomalies_df, recs_df, tx_df):
        df.columns = df.columns.astype(str).str.strip()

    # 4) Normalize key columns types early
    if "TransactionID" in anomalies_df.columns:
        anomalies_df["TransactionID"] = anomalies_df["TransactionID"].astype(str).str.strip()
    if "TransactionID" in tx_df.columns:
        tx_df["TransactionID"] = tx_df["TransactionID"].astype(str).str.strip()

    if "AccountID" in clusters_df.columns:
        clusters_df["AccountID"] = clusters_df["AccountID"].astype(str).str.strip()
    if "AccountID" in recs_df.columns:
        recs_df["AccountID"] = recs_df["AccountID"].astype(str).str.strip()
    if "AccountID" in tx_df.columns:
        tx_df["AccountID"] = tx_df["AccountID"].astype(str).str.strip()
    if "AccountID" in anomalies_df.columns:
        anomalies_df["AccountID"] = anomalies_df["AccountID"].astype(str).str.strip()

    # 5) Parse TransactionDate in transactions (source of truth)
    if "TransactionDate" in tx_df.columns:
        tx_df["TransactionDate"] = pd.to_datetime(tx_df["TransactionDate"], errors="coerce")

    # 6) IsAnomaly_bool
    if "IsAnomaly" in anomalies_df.columns:
        anomalies_df["IsAnomaly_bool"] = to_bool_series(anomalies_df["IsAnomaly"])
    else:
        anomalies_df["IsAnomaly_bool"] = False

    # 7) Patch AccountID if missing in anomalies.csv (merge from transactions)
    if "AccountID" not in anomalies_df.columns:
        if {"TransactionID", "AccountID"}.issubset(tx_df.columns) and "TransactionID" in anomalies_df.columns:
            anomalies_df = anomalies_df.merge(
                tx_df[["TransactionID", "AccountID"]].drop_duplicates("TransactionID"),
                on="TransactionID",
                how="left",
            )

    # 8) Merge context columns from transactions (prefer tx values)
    #    This prevents TransactionDate being string and fixes str vs Timestamp comparisons.
    context_cols = ["AccountID", "TransactionDate", "TransactionAmount", "TransactionType", "AccountBalance", "TransactionDuration", "LoginAttempts"]
    tx_keep = ["TransactionID"] + [c for c in context_cols if c in tx_df.columns]

    if "TransactionID" in anomalies_df.columns and "TransactionID" in tx_df.columns:
        merged = anomalies_df.merge(
            tx_df[tx_keep].drop_duplicates("TransactionID"),
            on="TransactionID",
            how="left",
            suffixes=("", "_tx"),
        )

        # Prefer transaction columns if present (_tx)
        for col in context_cols:
            tx_col = f"{col}_tx"
            if tx_col in merged.columns:
                # Use combine_first to fill missing values in original col with values from tx_col
                if col in merged.columns:
                    merged[col] = merged[col].combine_first(merged[tx_col])
                else:
                    merged[col] = merged[tx_col]
                merged.drop(columns=[tx_col], inplace=True)

        anomalies_merged = merged
    else:
        anomalies_merged = anomalies_df.copy()

    # 9) Force correct dtypes in anomalies_merged
    # TransactionDate must be datetime
    if "TransactionDate" in anomalies_merged.columns:
        anomalies_merged["TransactionDate"] = pd.to_datetime(anomalies_merged["TransactionDate"], errors="coerce")
    else:
        anomalies_merged["TransactionDate"] = pd.NaT

    # numeric conversions
    for c in ["TransactionAmount", "AccountBalance", "AnomalyScore", "TransactionDuration", "LoginAttempts"]:
        if c in anomalies_merged.columns:
            anomalies_merged[c] = pd.to_numeric(anomalies_merged[c], errors="coerce")

    # Ensure AccountID exists
    if "AccountID" in anomalies_merged.columns:
        anomalies_merged["AccountID"] = anomalies_merged["AccountID"].astype(str).str.strip()

    return base_path, clusters_df, anomalies_merged, recs_df, tx_df


base_path, clusters_df, anomalies_df, recommendations_df, transactions_df = load_data()


# =========================
# Schema checks (stability)
# =========================
if not require_columns(clusters_df, ["AccountID"], "outputs/clusters.csv"):
    st.stop()

# For anomalies: require minimal fields; AccountID may be patched by merge above
if not require_columns(anomalies_df, ["TransactionID", "IsAnomaly_bool", "AnomalyScore"], "outputs/anomalies.csv"):
    st.stop()

if "AccountID" not in anomalies_df.columns:
    st.error(
        "‚ùå `outputs/anomalies.csv` thi·∫øu `AccountID` v√† kh√¥ng th·ªÉ merge t·ª´ `bank_transactions_data_2.csv`.\n"
        "‚û°Ô∏è C·∫ßn ƒë·∫£m b·∫£o transactions c√≥ `AccountID` v√† anomalies c√≥ `TransactionID`."
    )
    st.stop()

if "RiskLevel" not in anomalies_df.columns:
    st.warning("‚ö†Ô∏è anomalies.csv ch∆∞a c√≥ c·ªôt RiskLevel. B·∫°n v·∫´n d√πng ƒë∆∞·ª£c app, nh∆∞ng filter RiskLevel s·∫Ω tr·ªëng.")


# =========================
# Title
# =========================
st.title("üè¶ Ph√¢n t√≠ch H·ªá th·ªëng T√†i ch√≠nh ML")
st.caption(
    "Dashboard ph√¢n t√≠ch giao d·ªãch ng√¢n h√†ng: Ph√¢n kh√∫c kh√°ch h√†ng (K-Means), "
    "Ph√°t hi·ªán b·∫•t th∆∞·ªùng (Isolation Forest), G·ª£i √Ω ti·∫øt ki·ªám (Rule-based)."
)


# =========================
# Sidebar filters
# =========================
st.sidebar.markdown("## üéõÔ∏è B·ªô l·ªçc")

# Cluster filter
cluster_values = sorted(clusters_df["ClusterID"].dropna().unique().tolist()) if "ClusterID" in clusters_df.columns else []
cluster_filter = st.sidebar.multiselect("Nh√≥m (Cluster)", options=cluster_values, default=cluster_values)

overview_anom_only = st.sidebar.toggle(
    "Overview: ch·ªâ t√≠nh IsAnomaly=True",
    value=True,
    help="B·∫≠t ƒë·ªÉ KPI/trend/top risky ch·ªâ t√≠nh giao d·ªãch b·∫•t th∆∞·ªùng th·∫≠t.",
)

# RiskLevel filter
risk_values = []
if "RiskLevel" in anomalies_df.columns:
    risk_values = sorted(anomalies_df["RiskLevel"].dropna().unique().tolist(), key=risk_level_order)
risk_filter = st.sidebar.multiselect("M·ª©c r·ªßi ro (RiskLevel)", options=risk_values, default=risk_values)

# TransactionType filter
type_values = []
if "TransactionType" in anomalies_df.columns:
    type_values = sorted(anomalies_df["TransactionType"].dropna().unique().tolist())
type_filter = st.sidebar.multiselect("Lo·∫°i giao d·ªãch (TransactionType)", options=type_values, default=type_values)

# Amount range filter
amount_range = None
if "TransactionAmount" in anomalies_df.columns:
    amt = pd.to_numeric(anomalies_df["TransactionAmount"], errors="coerce").dropna()
    if not amt.empty:
        amount_range = st.sidebar.slider(
            "Kho·∫£ng ti·ªÅn giao d·ªãch",
            min_value=float(amt.min()),
            max_value=float(amt.max()),
            value=(float(amt.min()), float(amt.max())),
            step=max((float(amt.max()) - float(amt.min())) / 100.0, 1.0),
        )

# Date range
min_dt, max_dt = safe_datetime_minmax(anomalies_df["TransactionDate"]) if "TransactionDate" in anomalies_df.columns else (None, None)
date_range = None
if min_dt is not None and max_dt is not None:
    date_range = st.sidebar.date_input(
        "Kho·∫£ng th·ªùi gian",
        value=(min_dt.date(), max_dt.date()),
        min_value=min_dt.date(),
        max_value=max_dt.date(),
    )

account_search = st.sidebar.text_input("T√¨m AccountID", value="")


# =========================
# Apply filters
# =========================
filtered_clusters_df = clusters_df.copy()
if cluster_filter and "ClusterID" in filtered_clusters_df.columns:
    filtered_clusters_df = filtered_clusters_df[filtered_clusters_df["ClusterID"].isin(cluster_filter)]

filtered_anoms_df = anomalies_df.copy()

# apply cluster->account constraint
allowed_accounts = set(filtered_clusters_df["AccountID"].astype(str).unique().tolist())
filtered_anoms_df["AccountID"] = filtered_anoms_df["AccountID"].astype(str)
filtered_anoms_df = filtered_anoms_df[filtered_anoms_df["AccountID"].isin(allowed_accounts)]

# risk/type/amount/date filters
if risk_filter and "RiskLevel" in filtered_anoms_df.columns:
    filtered_anoms_df = filtered_anoms_df[filtered_anoms_df["RiskLevel"].isin(risk_filter)]

if type_filter and "TransactionType" in filtered_anoms_df.columns:
    filtered_anoms_df = filtered_anoms_df[filtered_anoms_df["TransactionType"].isin(type_filter)]

if amount_range and "TransactionAmount" in filtered_anoms_df.columns:
    amt = pd.to_numeric(filtered_anoms_df["TransactionAmount"], errors="coerce")
    filtered_anoms_df = filtered_anoms_df[amt.between(amount_range[0], amount_range[1], inclusive="both")]

if date_range and len(date_range) == 2 and "TransactionDate" in filtered_anoms_df.columns:
    start = pd.to_datetime(date_range[0])
    end = pd.to_datetime(date_range[1]) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)
    filtered_anoms_df = filtered_anoms_df[
        filtered_anoms_df["TransactionDate"].notna()
        & (filtered_anoms_df["TransactionDate"] >= start)
        & (filtered_anoms_df["TransactionDate"] <= end)
    ]

filtered_true_anoms_df = filtered_anoms_df[filtered_anoms_df["IsAnomaly_bool"]].copy()

# account list
account_ids = sorted(filtered_clusters_df["AccountID"].dropna().astype(str).unique().tolist())
if account_search.strip():
    key = account_search.strip().upper()
    account_ids = [a for a in account_ids if key in str(a).upper()]


# =========================
# Tabs
# =========================
tab_overview, tab_customer, tab_goal = st.tabs(["üìä T·ªïng quan", "üë§ Kh√°ch h√†ng", "üéØ M·ª•c ti√™u ti·∫øt ki·ªám"])


# ==========================================================
# TAB OVERVIEW
# ==========================================================
with tab_overview:
    st.subheader("üìä T·ªïng quan h·ªá th·ªëng")
    overview_df = filtered_true_anoms_df if overview_anom_only else filtered_anoms_df

    c1, c2, c3, c4 = st.columns(4)
    total_customers = int(filtered_clusters_df["AccountID"].nunique()) if "AccountID" in filtered_clusters_df.columns else 0
    total_tx = int(transactions_df.shape[0]) if transactions_df is not None else 0
    total_rows = int(overview_df.shape[0]) if overview_df is not None else 0
    high_cnt = int((overview_df["RiskLevel"] == "High").sum()) if (not overview_df.empty and "RiskLevel" in overview_df.columns) else 0

    c1.metric("T·ªïng kh√°ch h√†ng", f"{total_customers}")
    c2.metric("T·ªïng giao d·ªãch", f"{total_tx}")
    c3.metric("S·ªë b·∫£n ghi (theo b·ªô l·ªçc)", f"{total_rows}")
    c4.metric("R·ªßi ro cao", f"{high_cnt}")

    st.caption("Ghi ch√∫: **AnomalyScore c√†ng th·∫•p (c√†ng √¢m) ‚áí c√†ng b·∫•t th∆∞·ªùng ‚áí r·ªßi ro cao h∆°n.**")

    st.divider()

    left, right = st.columns([1, 1])

    with left:
        st.markdown("### Ph√¢n b·ªë kh√°ch h√†ng theo Cluster")
        if {"ClusterID", "AccountID"}.issubset(filtered_clusters_df.columns):
            dist = (
                filtered_clusters_df.groupby("ClusterID")["AccountID"]
                .nunique().reset_index(name="CustomerCount")
                .sort_values("ClusterID")
            )
            st.bar_chart(dist.set_index("ClusterID")["CustomerCount"])
            st.dataframe(dist, use_container_width=True, hide_index=True)

    with right:
        st.markdown("### Xu h∆∞·ªõng b·∫•t th∆∞·ªùng theo th√°ng")
        if not overview_df.empty and "TransactionDate" in overview_df.columns and overview_df["TransactionDate"].notna().any():
            tmp = overview_df.copy()
            tmp["Month"] = tmp["TransactionDate"].dt.to_period("M").astype(str)
            if "RiskLevel" in tmp.columns:
                ts = tmp.groupby(["Month", "RiskLevel"]).size().reset_index(name="Count")
                pivot = ts.pivot(index="Month", columns="RiskLevel", values="Count").fillna(0).sort_index()
                st.line_chart(pivot)
            else:
                ts = tmp.groupby("Month").size().reset_index(name="Count").sort_values("Month")
                st.line_chart(ts.set_index("Month")["Count"])
        else:
            st.info("Kh√¥ng c√≥ TransactionDate ƒë·ªÉ v·∫Ω trend.")

    st.divider()

    st.markdown("### Top kh√°ch h√†ng r·ªßi ro nh·∫•t (ƒëi·ªÉm TB th·∫•p nh·∫•t)")
    if not overview_df.empty and {"AccountID", "AnomalyScore"}.issubset(overview_df.columns):
        agg = overview_df.groupby("AccountID").agg(
            AvgAnomalyScore=("AnomalyScore", "mean"),
            Count=("AnomalyScore", "count"),
        ).reset_index().sort_values("AvgAnomalyScore", ascending=True).head(10)

        if "RiskLevel" in overview_df.columns:
            high = overview_df[overview_df["RiskLevel"] == "High"].groupby("AccountID").size().reset_index(name="HighCount")
            agg = agg.merge(high, on="AccountID", how="left").fillna({"HighCount": 0})

        st.dataframe(agg, use_container_width=True, hide_index=True)
    else:
        st.info("Thi·∫øu c·ªôt AccountID/AnomalyScore ƒë·ªÉ t√≠nh top risky.")


# ==========================================================
# TAB CUSTOMER
# ==========================================================
with tab_customer:
    st.subheader("üë§ Chi ti·∫øt kh√°ch h√†ng")

    if not account_ids:
        st.warning("Kh√¥ng c√≥ AccountID ph√π h·ª£p v·ªõi b·ªô l·ªçc hi·ªán t·∫°i.")
        st.stop()

    selected_account = st.selectbox("Ch·ªçn AccountID", options=account_ids, key="cust_acc")

    # Safe fetch
    customer_cluster = safe_get_row(clusters_df.astype({"AccountID": str}), "AccountID", str(selected_account))
    customer_rec = safe_get_row(recommendations_df.astype({"AccountID": str}), "AccountID", str(selected_account))

    cust_all = filtered_anoms_df[filtered_anoms_df["AccountID"] == str(selected_account)].copy()
    cust_true = cust_all[cust_all["IsAnomaly_bool"]].copy()

    # KPI cards
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("Cluster", f"{int(customer_cluster['ClusterID'])}" if (customer_cluster is not None and "ClusterID" in customer_cluster) else "N/A")
    k2.metric("S·ªë d∆∞ TB", f"${float(customer_cluster['average_account_balance']):.2f}" if (customer_cluster is not None and "average_account_balance" in customer_cluster) else "N/A")
    k3.metric("T·∫ßn su·∫•t GD", f"{int(float(customer_cluster['transaction_frequency']))}" if (customer_cluster is not None and "transaction_frequency" in customer_cluster) else "N/A")
    k4.metric("S·ªë GD nghi v·∫•n", f"{len(cust_true)}")

    st.divider()

    # --- BALANCE TREND ---
    trend = compute_balance_trend(transactions_df, selected_account)
    st.markdown("### üìâ Xu h∆∞·ªõng s·ªë d∆∞ (Linear Regression)")
    if trend is None:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu s·ªë d∆∞ theo th·ªùi gian ƒë·ªÉ fit trend (c·∫ßn √≠t nh·∫•t 2 ng√†y c√≥ balance).")
    else:
        c1, c2, c3, c4 = st.columns(4)
        c1.metric("S·ªë d∆∞ g·∫ßn nh·∫•t", f"${trend['last_balance']:,.2f}")
        c2.metric("Slope / th√°ng", f"{trend['slope_per_month']:+.2f}")
        c3.metric("D·ª± b√°o +30 ng√†y", f"${trend['forecast_30d']:,.2f}")
        c4.metric("R¬≤ (ƒê·ªô tin c·∫≠y)", f"{trend['r2']:.2f}")

        if trend["slope_per_month"] < 0:
            st.warning("Xu h∆∞·ªõng s·ªë d∆∞ ƒëang GI·∫¢M. C√≥ d·∫•u hi·ªáu ti√™u l·∫°m v√†o v·ªën n·∫øu kh√¥ng ƒëi·ªÅu ch·ªânh.")
        else:
            st.success("Xu h∆∞·ªõng s·ªë d∆∞ ƒëang TƒÇNG ho·∫∑c ·ªïn ƒë·ªãnh. T√¨nh h√¨nh t√†i ch√≠nh t√≠ch c·ª±c.")

        s = trend["daily_series"].set_index("Date")["Balance"]
        st.line_chart(s)
    st.divider()


    # --- CHARTS ---
    st.markdown("### üìà Bi·ªÉu ƒë·ªì (Kh√°ch h√†ng)")
    colA, colB = st.columns([1.4, 1])

    with colA:
        st.markdown("**Timeline giao d·ªãch (Amount theo th·ªùi gian) + highlight anomalies**")
        if {"TransactionDate", "TransactionAmount"}.issubset(cust_all.columns) and cust_all["TransactionDate"].notna().any():
            tmp = cust_all.copy()
            tmp["TransactionAmount"] = pd.to_numeric(tmp["TransactionAmount"], errors="coerce")
            tmp = tmp.dropna(subset=["TransactionDate", "TransactionAmount"])

            if px is not None and not tmp.empty:
                fig = px.scatter(
                    tmp.sort_values("TransactionDate"),
                    x="TransactionDate",
                    y="TransactionAmount",
                    color="IsAnomaly_bool",
                    hover_data=[c for c in ["TransactionID", "RiskLevel", "TransactionType", "AnomalyScore"] if c in tmp.columns],
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                if not tmp.empty:
                    st.line_chart(tmp.set_index("TransactionDate")["TransactionAmount"])
                st.caption("Plotly kh√¥ng kh·∫£ d·ª•ng -> d√πng chart c∆° b·∫£n.")
        else:
            st.info("Thi·∫øu TransactionDate/TransactionAmount ƒë·ªÉ v·∫Ω timeline.")

    with colB:
        st.markdown("**Ph√¢n ph·ªëi AnomalyScore: Kh√°ch vs To√†n h·ªá th·ªëng**")
        if "AnomalyScore" in filtered_anoms_df.columns:
            global_scores = filtered_anoms_df["AnomalyScore"].dropna()
            cust_scores = cust_all["AnomalyScore"].dropna()

            if px is not None and not global_scores.empty and not cust_scores.empty:
                df_plot = pd.DataFrame({
                    "AnomalyScore": pd.concat([cust_scores, global_scores], ignore_index=True),
                    "Group": (["Customer"] * len(cust_scores)) + (["Global"] * len(global_scores))
                })
                fig2 = px.histogram(df_plot, x="AnomalyScore", color="Group", nbins=40, barmode="overlay")
                st.plotly_chart(fig2, use_container_width=True)
            else:
                st.write("Customer score count:", len(cust_scores), " | Global score count:", len(global_scores))
        else:
            st.info("Thi·∫øu AnomalyScore ƒë·ªÉ v·∫Ω ph√¢n ph·ªëi.")

    st.divider()

    st.markdown("### üìä So s√°nh Kh√°ch vs Trung b√¨nh Cluster")
    if customer_cluster is not None and "ClusterID" in customer_cluster and "ClusterID" in clusters_df.columns:
        cid = int(customer_cluster["ClusterID"])
        peers = clusters_df[clusters_df["ClusterID"] == cid]

        metrics = [
            ("average_account_balance", "Avg Balance"),
            ("transaction_frequency", "Txn Frequency"),
            ("average_transaction_amount", "Avg Txn Amount"),
        ]
        rows = []
        for k, label in metrics:
            if k in clusters_df.columns and k in customer_cluster and not peers.empty:
                cust_val = pd.to_numeric(customer_cluster.get(k), errors='coerce')
                peer_mean = pd.to_numeric(peers[k], errors='coerce').mean()
                if not pd.isna(cust_val) and not pd.isna(peer_mean):
                    rows.append({"Metric": label, "Customer": cust_val, "ClusterMean": peer_mean})

        if rows:
            df_cmp = pd.DataFrame(rows)
            if px is not None:
                fig3 = px.bar(
                    df_cmp.melt(id_vars="Metric", var_name="Group", value_name="Value"),
                    x="Metric", y="Value", color="Group", barmode="group"
                )
                st.plotly_chart(fig3, use_container_width=True)
            else:
                st.dataframe(df_cmp, use_container_width=True, hide_index=True)
        else:
            st.info("Thi·∫øu feature ƒë·ªÉ so s√°nh customer vs cluster.")
    else:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu cluster ƒë·ªÉ so s√°nh.")

    st.divider()

    st.markdown("### üßæ Danh s√°ch giao d·ªãch (Drill-down)")
    show_only_anomalies = st.toggle("Ch·ªâ hi·ªÉn th·ªã IsAnomaly=True", value=True, key="cust_anom_toggle")
    table_df = cust_true if show_only_anomalies else cust_all

    base_cols = ["TransactionID", "TransactionDate", "TransactionAmount", "TransactionType", "RiskLevel", "AnomalyScore", "IsAnomaly_bool"]
    cols_exist = [c for c in base_cols if c in table_df.columns]

    if show_only_anomalies and "AnomalyScore" in table_df.columns:
        table_df = table_df.sort_values("AnomalyScore", ascending=True)

    st.dataframe(table_df[cols_exist].head(200), use_container_width=True, hide_index=True)
    st.caption("Hi·ªÉn th·ªã t·ªëi ƒëa 200 d√≤ng. D√πng b·ªô l·ªçc ·ªü sidebar ƒë·ªÉ thu h·∫πp d·ªØ li·ªáu.")
    st.divider()

    # --- DEVIATION EXPLANATION ---
    st.markdown("### üîé Gi·∫£i th√≠ch v√¨ sao b·ªã c·∫£nh b√°o (Deviation Explanation)")
    if cust_true.empty:
        st.info("Kh√¥ng c√≥ anomaly ƒë·ªÉ gi·∫£i th√≠ch.")
    else:
        pick_txid = st.selectbox("Ch·ªçn TransactionID ƒë·ªÉ xem gi·∫£i th√≠ch", options=cust_true["TransactionID"].astype(str).tolist())
        row = cust_true[cust_true["TransactionID"].astype(str) == str(pick_txid)].iloc[0]

        hist = transactions_df[transactions_df["AccountID"] == str(selected_account)].copy()
        expl = deviation_explanation(row, hist, top_k=3)

        if not expl:
            st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ gi·∫£i th√≠ch theo deviation.")
        else:
            st.caption("So s√°nh giao d·ªãch n√†y v·ªõi h√†nh vi l·ªãch s·ª≠ c·ªßa ch√≠nh kh√°ch h√†ng ƒë√≥:")
            for e in expl:
                st.write(f"- **{e['feature']}** = `{e['value']:.2f}` ‚Üí {e['reason']} (z-score = {e['z']:.2f})")
    st.divider()


    # Downloads
    st.markdown("### üì• T·∫£i d·ªØ li·ªáu")
    d1, d2, d3 = st.columns(3)
    with d1:
        st.download_button(
            "‚¨áÔ∏è T·∫£i anomalies c·ªßa kh√°ch (CSV)",
            data=csv_bytes(cust_true if not cust_true.empty else cust_all),
            file_name=f"anomalies_{selected_account}.csv",
            mime="text/csv"
        )
    with d2:
        prof = pd.DataFrame([customer_cluster]) if customer_cluster is not None else pd.DataFrame()
        st.download_button(
            "‚¨áÔ∏è T·∫£i profile (CSV)",
            data=csv_bytes(prof) if not prof.empty else b"",
            file_name=f"profile_{selected_account}.csv",
            mime="text/csv",
            disabled=prof.empty
        )
    with d3:
        rec = pd.DataFrame([customer_rec]) if customer_rec is not None else pd.DataFrame()
        st.download_button(
            "‚¨áÔ∏è T·∫£i recommendation (CSV)",
            data=csv_bytes(rec) if not rec.empty else b"",
            file_name=f"recommendation_{selected_account}.csv",
            mime="text/csv",
            disabled=rec.empty
        )

    st.divider()

    # Feedback loop
    with st.expander("üß† Feedback (Human-in-the-loop)", expanded=False):
        st.caption("Tick c√°c giao d·ªãch b·∫•t th∆∞·ªùng nh∆∞ng b·∫°n cho r·∫±ng **kh√¥ng ph·∫£i gian l·∫≠n** ƒë·ªÉ l∆∞u ph·∫£n h·ªìi.")
        if cust_true.empty:
            st.info("Kh√°ch n√†y kh√¥ng c√≥ giao d·ªãch b·∫•t th∆∞·ªùng ƒë·ªÉ feedback.")
        else:
            fb_cols = ["TransactionID", "TransactionDate", "TransactionAmount", "TransactionType", "RiskLevel", "AnomalyScore"]
            fb_cols = [c for c in fb_cols if c in cust_true.columns]
            fb_view = cust_true[fb_cols].copy().head(50)
            fb_view["NotFraud"] = False

            edited = st.data_editor(
                fb_view,
                use_container_width=True,
                hide_index=True,
                column_config={"NotFraud": st.column_config.CheckboxColumn("Kh√¥ng ph·∫£i gian l·∫≠n", default=False)},
                disabled=[c for c in fb_view.columns if c != "NotFraud"]
            )

            if st.button("üíæ L∆∞u ph·∫£n h·ªìi"):
                picked = edited[edited["NotFraud"] == True]
                if picked.empty:
                    st.warning("B·∫°n ch∆∞a tick giao d·ªãch n√†o.")
                else:
                    rows = [{
                        "CreatedAt": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        "AccountID": str(selected_account),
                        "TransactionID": str(r.get("TransactionID", "")),
                        "Reason": "User marked as Not Fraud"
                    } for _, r in picked.iterrows()]

                    append_feedback(base_path, rows)
                    st.success(f"ƒê√£ l∆∞u {len(rows)} ph·∫£n h·ªìi v√†o outputs/feedback.csv")
    st.divider()

    # Recommendation
    st.markdown("### üí∞ G·ª£i √Ω ti·∫øt ki·ªám (Rule-based)")
    if customer_rec is None:
        st.info("Kh√¥ng c√≥ recommendation cho kh√°ch h√†ng n√†y.")
    else:
        prio = str(customer_rec.get("PriorityLevel", "MEDIUM")).upper()
        msg = str(customer_rec.get("RecommendationMessage", "")).strip()
        reason = str(customer_rec.get("RecommendationReason", "")).strip()
        cat = str(customer_rec.get("RecommendationCategory", "")).strip()
        try:
            savings = float(customer_rec.get("SavingsPotential", 0.0))
        except Exception:
            savings = 0.0

        if prio == "HIGH":
            st.error(f"**∆Øu ti√™n: CAO** ‚Äî {msg}")
        elif prio == "MEDIUM":
            st.warning(f"**∆Øu ti√™n: TRUNG B√åNH** ‚Äî {msg}")
        else:
            st.success(f"**∆Øu ti√™n: TH·∫§P** ‚Äî {msg}")

        if cat: st.markdown(f"**Danh m·ª•c:** {cat}")
        if reason: st.markdown(f"**L√Ω do:** {reason}")
        if savings > 0:
            st.markdown("**ƒêi·ªÉm ti·ªÅm nƒÉng ti·∫øt ki·ªám:**")
            st.progress(min(max(savings / 100.0, 0.0), 1.0))
            st.caption(f"Ti·ªÅm nƒÉng ti·∫øt ki·ªám: {savings:.2f}/100")


# ==========================================================
# TAB GOAL-BASED SAVINGS
# ==========================================================
with tab_goal:
    st.subheader("üéØ Ti·∫øt ki·ªám theo m·ª•c ti√™u (Goal-based)")

    if not account_ids:
        st.warning("Kh√¥ng c√≥ AccountID ph√π h·ª£p v·ªõi b·ªô l·ªçc sidebar.")
        st.stop()

    acc = st.selectbox("Ch·ªçn AccountID", options=account_ids, key="goal_acc")
    trend = compute_balance_trend(transactions_df, acc)

    c1, c2 = st.columns(2)
    with c1:
        goal_name = st.text_input("T√™n m·ª•c ti√™u", value="Mua Laptop Gaming")
        target_amount = st.number_input("S·ªë ti·ªÅn m·ª•c ti√™u (VND)", min_value=0.0, value=40_000_000.0, step=1_000_000.0)
        deadline = st.date_input("H·∫°n ch√≥t", value=(datetime.now() + pd.Timedelta(days=365)).date())
    with c2:
        default_current = float(trend["last_balance"]) if trend else 0.0
        current_amount = st.number_input(
            "S·ªë ti·ªÅn ƒë√£ c√≥ (m·∫∑c ƒë·ªãnh l·∫•y s·ªë d∆∞ g·∫ßn nh·∫•t)",
            min_value=0.0,
            value=default_current,
            step=1_000_000.0
        )
        st.caption(f"S·ªë d∆∞ g·∫ßn nh·∫•t c·ªßa t√†i kho·∫£n `{acc}` l√† `{default_current:,.0f}`.")

    st.divider()

    days_left = (pd.to_datetime(deadline) - pd.to_datetime(datetime.now().date())).days
    if days_left <= 0:
        st.error("H·∫°n ch√≥t ph·∫£i ·ªü trong t∆∞∆°ng lai.")
        st.stop()

    months_left = max(int(np.ceil(days_left / 30.0)), 1)
    gap = max(target_amount - current_amount, 0.0)
    need_per_month = gap / months_left

    st.markdown("#### K·∫ø ho·∫°ch ti·∫øt ki·ªám")
    gc1, gc2, gc3 = st.columns(3)
    gc1.metric("S·ªë ti·ªÅn c√≤n thi·∫øu", f"{gap:,.0f} VND")
    gc2.metric("S·ªë th√°ng c√≤n l·∫°i", f"{months_left}")
    gc3.metric("C·∫ßn ti·∫øt ki·ªám / th√°ng", f"{need_per_month:,.0f} VND")

    st.divider()
    st.markdown("#### ƒê√°nh gi√° t√≠nh kh·∫£ thi")
    if trend:
        expected_monthly_change = trend["slope_per_month"]
        st.caption(f"D·ª±a tr√™n ph√¢n t√≠ch, s·ªë d∆∞ c·ªßa b·∫°n ƒëang thay ƒë·ªïi trung b√¨nh ‚âà **{expected_monthly_change:+,.0f} VND/th√°ng** (R¬≤={trend['r2']:.2f}).")

        if expected_monthly_change < need_per_month:
            st.warning(f"**KH√ì KH·∫¢ THI.** Xu h∆∞·ªõng hi·ªán t·∫°i c·ªßa b·∫°n ({expected_monthly_change:+,.0f}) th·∫•p h∆°n m·ª©c c·∫ßn ti·∫øt ki·ªám ({need_per_month:,.0f}).")
            st.markdown("G·ª£i √Ω: TƒÉng thu nh·∫≠p, gi·∫£m chi ti√™u, ho·∫∑c k√©o d√†i h·∫°n ch√≥t.")
        else:
            st.success(f"**KH·∫¢ THI.** Xu h∆∞·ªõng hi·ªán t·∫°i c·ªßa b·∫°n ({expected_monthly_change:+,.0f}) cao h∆°n m·ª©c c·∫ßn ti·∫øt ki·ªám ({need_per_month:,.0f}).")
            st.balloons()
    else:
        st.info("Kh√¥ng c√≥ d·ªØ li·ªáu xu h∆∞·ªõng s·ªë d∆∞ ƒë·ªÉ ƒë√°nh gi√° t√≠nh kh·∫£ thi.")

    st.divider()
    st.markdown("#### G·ª£i √Ω h√†nh ƒë·ªông")
    st.info(
        "üí° **M·∫πo:** C·∫Øt gi·∫£m c√°c kho·∫£n chi b·∫•t th∆∞·ªùng (anomalies) v√† c√°c kho·∫£n chi l·ªõn kh√¥ng c·∫ßn thi·∫øt. "
        "Thi·∫øt l·∫≠p l·ªánh chuy·ªÉn ti·ªÅn t·ª± ƒë·ªông v√†o t√†i kho·∫£n ti·∫øt ki·ªám v√†o ƒë·∫ßu m·ªói th√°ng."
    )


st.markdown(
    """
---
**G·ª£i √Ω n·∫øu thi·∫øu output:**
- Regenerate b·∫±ng pipeline/notebook:
  - `notebooks/03_customer_clustering.ipynb` (clusters)
  - `notebooks/04_fraud_detection_fixed.ipynb` (anomalies)
  - `notebooks/05_saving_recommendation.ipynb` (recommendations)
"""
)
